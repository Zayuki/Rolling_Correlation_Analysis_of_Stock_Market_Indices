from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import pandas as pd
import numpy as np

spark = SparkSession.builder.getOrCreate()

Dataschema1 = StructType([\
    StructField("Index", StringType(), True),\
    StructField("YearMonth", StringType(), True),\
    StructField("AvgClose", FloatType(), True)])

df = spark.read.format("csv").option("header", "false").schema(Dataschema1).load("/home/vclee/Downloads/hive_monthly/000000_0")
df.show(5)

pivotdf = df.groupBy("YearMonth").pivot("Index").sum("AvgClose")
pivotdf.show(5)

listcolumn = ["`000001.SS`", "`399001.SZ`", "GDAXI", "GSPTSE", "HSI", "IXIC", "`J203.JO`", "KS11", "N100", "N225", "NSEI", "NYA", "SSMI", "TWII"]
listcolumnpandas = ["000001.SS", "399001.SZ", "GDAXI", "GSPTSE", "HSI", "IXIC", "J203.JO", "KS11", "N100", "N225", "NSEI", "NYA", "SSMI", "TWII"]


def extracttwoindices(indexnumber1, indexnumber2):
	index1 = listcolumn[indexnumber1]
	index2 = listcolumn[indexnumber2]
	index1pandas = listcolumnpandas[indexnumber1]
	index2pandas = listcolumnpandas[indexnumber2]
	dffinal = pivotdf.select("YearMonth", index1, index2).filter(col(index1).isNotNull() & col(index2).isNotNull()).sort("YearMonth")
	dffinalpandas = dffinal.toPandas()
	dffinalpandas['RollingCor12m'] = dffinalpandas[index1pandas].rolling(12).corr(dffinalpandas[index2pandas])
	dffinalpandas['RollingCor24m'] = dffinalpandas[index1pandas].rolling(24).corr(dffinalpandas[index2pandas])
	dffinalpandas['RollingCor36m'] = dffinalpandas[index1pandas].rolling(36).corr(dffinalpandas[index2pandas])
	dffinal1 = spark.createDataFrame(dffinalpandas)
	path = "/home/vclee/Downloads/hive_monthly/" + str(indexnumber1) + "_" + str(indexnumber2) + ".csv"
	dffinal1.write.option("header", "true").csv(path)
	return 'done'

for x in range(0, 14):
	for y in range (x+1, 14):
		extracttwoindices(x, y)